{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting a Burglary and Finding the Best Model with Cost Benefit Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns',500)\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Import Data from a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://data63206330.file.core.windows.net/data6320/CantonPoliceDept_HW05.csv?sp=rl&st=2021-02-17T17:59:40Z&se=2023-06-18T17:59:00Z&sv=2020-02-10&sig=OByF%2BPfEuCHPMSlspflhxezpcRUecv3bqqSNSn1Kpp8%3D&sr=f'\n",
    "df_all=pd.read_csv(url1, index_col=0, header=0)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://data63206330.file.core.windows.net/data6320/CantonPoliceDept_HW05_Week49.csv?sp=rl&st=2021-02-17T18:33:51Z&se=2023-06-18T18:33:00Z&sv=2020-02-10&sig=UnolmTRuVCs1mzx%2FyciUlEW8WntrkPdkg5njluGfi%2BM%3D&sr=f'\n",
    "df_canton_new=pd.read_csv(url2, index_col=0, header=0)\n",
    "df_canton_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Create the Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.1 Create the X and y Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_all['BurgStatus']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_all.drop(['Subzone', 'YEAR_WEEK', 'SUB_YEAR_WEEK', 'All_comp',\n",
    "       'FalseAlarm_comp', 'Arrest_comp', 'Cleared_comp', 'NoContact_comp',\n",
    "       'NoReport_comp', 'Resolved_comp', 'Filed_comp', 'Calls_comp',\n",
    "       'BurgAlarm_comp', 'Suspicious_comp', 'Shots_comp', 'Intox_comp',\n",
    "       'Drugs_comp', 'Assault_comp', 'Armed_comp', 'Disturb_comp',\n",
    "       'Fireworks_comp', 'Noise_comp', 'Stalking_comp', 'ActualBurg',\n",
    "       'BurgStatus', 'BurgStatus2','Friday', 'Monday', 'Saturday',\n",
    "       'Sunday', 'Thursday', 'Tuesday', 'Wednesday', 'month_1', 'month_2',\n",
    "       'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
    "       'month_9', 'month_10', 'month_11', 'month_12','call_FW FIREWORKS','disp_Runaway juvenile (entered NCIC)',\n",
    "       'disp_SAT-SETTLED AMONG SELVES', 'disp_TES-TEST',\n",
    "       'disp_TI -TOW IN', 'disp_Truancy', 'disp_VA Hospital Alarm (Fire)',],axis=1)\n",
    "print(X.shape)\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2 - Create the Training and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# Split the data for training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = preprocessing.StandardScaler()\n",
    "\n",
    "# Scale the data using Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 - Classification using Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.1 - Using Random Forest create a model using the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeltraintest(vartrain, vartest, y_train, y_test, model):\n",
    "\n",
    "    #1) Set the properties for the model (model) - by setting vartrain, vartest, and model\n",
    "    \n",
    "    #2) Fit the model with training data\n",
    "    model.fit(vartrain, y_train)\n",
    "\n",
    "    #3) Predict the target variable with test data\n",
    "    model_pred = model.predict(vartest)\n",
    "    model_prob = model.predict_proba(vartest)\n",
    "\n",
    "    #4) Assess the accuracy with the test data\n",
    "    score = model.score(vartest, y_test)\n",
    "\n",
    "    print('XXXXXXXXXXXXXXXX ACCURACY SCORE XXXXXXXXXXXXXXXXXX')\n",
    "    print(round(score, 6))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    print('XXXXXXXXXXXXXXXX CONFUSION MATRIX XXXXXXXXXXXXXXXX')\n",
    "    print(confusion_matrix(y_test, model_pred))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    print('XXXXXXXXXXXXXX CLASSIFICATION REPORT XXXXXXXXXXXXXX')\n",
    "    print(classification_report(y_test, model_pred))\n",
    "    print('')\n",
    "\n",
    "\n",
    "    print('XXXXXXXXXXXXXX ROC AUC SCORE AND CHART XXXXXXXXXXXXXXXXXX')\n",
    "    print('')\n",
    "    y_pred_prob = model.predict_proba(vartest)[:,1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1],'k--')\n",
    "    plt.plot(fpr, tpr, label='Classification Model')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.show();\n",
    "\n",
    "    # calculate roc curve\n",
    "    y_pred_prob = model.predict_proba(vartest)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    roc_auc_format = 'ROC AUC Score: {0:.4f}'.format(roc_auc)\n",
    "    print(roc_auc_format)\n",
    "    print('')\n",
    "\n",
    "\n",
    "    print('XXXXXXXXXXXXXX CROSS VALIDATION XXXXXXXXXXXXXXXXXX')\n",
    "    print('')\n",
    "    cv_scores = cross_val_score(model, vartrain, y_train, cv=5,\n",
    "    scoring='accuracy')\n",
    "    print('CV Accuracy Scores:')\n",
    "    print(cv_scores)\n",
    "    print('')\n",
    "    cv_rocauc = cross_val_score(model, vartrain, y_train, cv=5,\n",
    "    scoring='roc_auc')\n",
    "    print('CV ROC AUC:')\n",
    "    print(cv_rocauc)\n",
    "\n",
    "    print('')\n",
    "    print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorttraintest(vartrain, vartest, y_train, y_test, model):\n",
    "\n",
    "    #Fit the model\n",
    "    model.fit(vartrain, y_train)\n",
    "\n",
    "    #Predict with the model\n",
    "    model_pred = model.predict(vartest)\n",
    "    model_prob = model.predict_proba(vartest)\n",
    "\n",
    "\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, model_pred))\n",
    "    print(\"\")\n",
    "\n",
    "    #Assess with the model\n",
    "    score = model.score(vartest, y_test)\n",
    "    score_format = 'Accuracy Score: {0:.4f}'.format(score)\n",
    "    print(score_format)\n",
    "\n",
    "    recall = recall_score(y_test, model_pred)\n",
    "    recall_format = 'Recall Score: {0:.4f}'.format(recall)\n",
    "    print(recall_format)\n",
    "    \n",
    "    precision = precision_score(y_test, model_pred)\n",
    "    precision_format = 'Precision Score: {0:.4f}'.format(precision)\n",
    "    print(precision_format)\n",
    "    \n",
    "    # calculate roc curve\n",
    "    y_pred_prob = model.predict_proba(vartest)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    roc_auc_format = 'ROC AUC Score: {0:.4f}'.format(roc_auc)\n",
    "    print(roc_auc_format)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "model_raw = RandomForestClassifier(random_state=21)\n",
    "\n",
    "modeltraintest(vartrain, vartest, y_train, y_test, model_raw)\n",
    "\n",
    "# Random forest classifier with default parameters. \n",
    "# Goal is to have accuracy score as close to 1 as possible. 81% is decent, but not good.\n",
    "# Note that recall score needs to be improved.\n",
    "# Note that folds hover around 81%. Comfortable enough with data to move on.\n",
    "# Since the folds are mostly even - with no outliers - the data is worthy of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "model_raw = RandomForestClassifier(random_state=21)\n",
    "\n",
    "shorttraintest(vartrain, vartest, y_train, y_test, model_raw)\n",
    "\n",
    "# More concise viewing of data, showing confusion matrix as well as scores from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "model_raw = RandomForestClassifier(random_state=21)\n",
    "\n",
    "shorttraintest(vartrain, vartrain, y_train, y_train, model_raw)\n",
    "\n",
    "# Set test data to training data to see overfit training data. Shows that parameters need to be set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.2 - Fine-tune the Model to Find the Optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#depth = range(6,25)\n",
    "\n",
    "#Chose to use a list instead of a rnage\n",
    "depth = [6, 8, 10, 12, 14, 16, 17, 18, 19, 20, 21, 22, 23, 30]\n",
    "\n",
    "#Creates an empty list\n",
    "scores = []\n",
    "\n",
    "for d in depth:\n",
    "    classifier=RandomForestClassifier(max_depth = d, random_state = 21)\n",
    "    classifier=classifier.fit(X_train,y_train)\n",
    "    score = classifier.score(X_test, y_test)\n",
    "    scores.append(classifier.score(X_test, y_test))\n",
    "    print(\"iteration {} done. Accuracy = \".format(d) + str(score))\n",
    "\n",
    "\n",
    "plt.plot(depth, scores, '-o')\n",
    "plt.xlabel('depth, d')\n",
    "plt.ylabel('scores')\n",
    "plt.xticks(depth)\n",
    "plt.show()\n",
    "\n",
    "# run for loops to see a set of random forest to see how many decision trees are needed.\n",
    "# note that iteration 18 shows the highest accuracy therefore is the best depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "model_opt = RandomForestClassifier(max_depth=18, random_state=21)\n",
    "\n",
    "shorttraintest(vartrain, vartest, y_train, y_test, model_opt)\n",
    "\n",
    "# Run a new short train with set max depth to 18.\n",
    "# Note results are shown quicker. Computer is thankful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "model_opt = RandomForestClassifier(max_depth=18, random_state=21)\n",
    "\n",
    "shorttraintest(vartrain, vartrain, y_train, y_train, model_opt)\n",
    "\n",
    "# New short train to test for overfit. Note that the new test is slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(20, 5)\n",
    "maxf = range(1,25)\n",
    "scores = []\n",
    "\n",
    "for d in maxf:\n",
    "    classifier=RandomForestClassifier(max_depth = 18, max_features = d, random_state=21)\n",
    "    classifier=classifier.fit(X_train,y_train)\n",
    "    score = classifier.score(X_test, y_test)\n",
    "    scores.append(classifier.score(X_test, y_test))\n",
    "    print(\"iteration {} done. Accuracy = \".format(d) + str(score))\n",
    "\n",
    "\n",
    "plt.plot(maxf, scores, '-o')\n",
    "plt.xlabel('maxf, d')\n",
    "plt.ylabel('scores')\n",
    "plt.xticks(maxf)\n",
    "plt.show()\n",
    "\n",
    "# run randomforest classifier using the default setting for max features and 18 for max depth.\n",
    "# note that around iteration 14 the differences in values is minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(20, 5)\n",
    "est = [1, 20, 50, 100, 150, 500]\n",
    "scores = []\n",
    "\n",
    "for d in est:\n",
    "    classifier=RandomForestClassifier(max_depth = 18, max_features = 14, n_estimators = d, random_state=21)\n",
    "    classifier=classifier.fit(X_train,y_train)\n",
    "    score = classifier.score(X_test, y_test)\n",
    "    scores.append(classifier.score(X_test, y_test))\n",
    "    print(\"iteration {} done. Accuracy = \".format(d) + str(score))\n",
    "\n",
    "\n",
    "plt.plot(est, scores, '-o')\n",
    "plt.xlabel('est, d')\n",
    "plt.ylabel('scores')\n",
    "plt.xticks(est)\n",
    "plt.show()\n",
    "\n",
    "# new test with set max features to 14 and n_estimators set to default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "model_opt = RandomForestClassifier(max_depth = 18, max_features = 14, n_estimators = 50, random_state=21)\n",
    "\n",
    "shorttraintest(vartrain, vartest, y_train, y_test, model_opt)\n",
    "\n",
    "# Short test with n_estimators set to 50. Takes longer than 20, but is slightly more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "model_opt = RandomForestClassifier(max_depth = 18, max_features = 14, n_estimators = 50, class_weight=None)\n",
    "\n",
    "shorttraintest(vartrain, vartrain, y_train, y_train, model_opt)\n",
    "\n",
    "# Data is still overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "model_opt = RandomForestClassifier(max_depth = 18, max_features = 14, n_estimators = 50, class_weight='balanced')\n",
    "\n",
    "shorttraintest(vartrain, vartest, y_train, y_test, model_opt)\n",
    "\n",
    "# Run test with class weight set to balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = [None, 'balanced', {0:1, 1:2}, {0:1, 1:3}, {0:1, 1:5}, {0:1, 1:10}, \n",
    "      {0:1, 1:20}, {0:1, 1:25}, {0:1, 1:30}, {0:1, 1:50}, {0:1, 1:100}]\n",
    "\n",
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "\n",
    "for w in cw:\n",
    "    print('----------------------')\n",
    "    vartitle = \"Model with Class Weight: \" + str(w) \n",
    "    varcw = w\n",
    "    model = RandomForestClassifier(max_depth = 18, max_features = 14, n_estimators = 50, class_weight=varcw)\n",
    "    print(vartitle)\n",
    "    print('')\n",
    "    shorttraintest(vartrain, vartest, y_train, y_test, model)\n",
    "\n",
    "\n",
    "          \n",
    "print('----------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "model_opt = RandomForestClassifier(max_depth = 18, max_features = 14, n_estimators = 50, class_weight={0: 1, 1: 10})\n",
    "\n",
    "modeltraintest(vartrain, vartest, y_train, y_test, model_opt)\n",
    "\n",
    "# run random forest using manually set properties. \n",
    "# chose class weight 0: 1, 1: 10 due to best combined scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "\n",
    "\n",
    "\n",
    "grid={\"criterion\": ['gini', 'entropy'], \"max_depth\" : [10, 13, 15, 17, 19, 21, 23],\n",
    "      \"n_estimators\" : [20], \"max_features\" : [10, 11, 13, 14, 17],\n",
    "      \"class_weight\": [None]}\n",
    "model_random = RandomForestClassifier(random_state=21)\n",
    "model_cv=RandomizedSearchCV(model_random,grid,cv=5)\n",
    "model_cv.fit(vartrain,y_train)\n",
    "\n",
    "print(\"tuned hyperparameters :(best parameters) \",model_cv.best_params_)\n",
    "print(\"accuracy :\", model_cv.best_score_)\n",
    "\n",
    "# Do a randomized search with n_estimators = 20 for time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "\n",
    "\n",
    "\n",
    "grid={\"criterion\": ['gini', 'entropy'], \"max_depth\" : [10, 13, 15, 17, 19, 21, 23],\n",
    "      \"n_estimators\" : [20], \"max_features\" : [10, 11, 13, 14, 17],\n",
    "      \"class_weight\": [None]}\n",
    "model_random = RandomForestClassifier(random_state=21, n_jobs=-2)\n",
    "model_cv=RandomizedSearchCV(model_random,grid,cv=5)\n",
    "model_cv.fit(vartrain,y_train)\n",
    "\n",
    "print(\"tuned hyperparameters :(best parameters) \",model_cv.best_params_)\n",
    "print(\"accuracy :\", model_cv.best_score_)\n",
    "\n",
    "# include n_jobs of a negative value if you want to save processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid={\"max_depth\" : [9, 11, 13, 15, 19, 21], \"criterion\": ['gini', 'entropy'],\n",
    "      \"n_estimators\" : [20], \"max_features\" : [11, 13, 17, 21],\n",
    "      \"class_weight\": [None]}\n",
    "model_grid = RandomForestClassifier(random_state=21, n_jobs=-2)\n",
    "model_cv=GridSearchCV(model_grid,grid,cv=5)\n",
    "model_cv.fit(vartrain,y_train)\n",
    "\n",
    "\n",
    "print(\"tuned hyperparameters :(best parameters) \",model_cv.best_params_)\n",
    "print(\"accuracy :\",model_cv.best_score_)\n",
    "\n",
    "# further tune parameters to create the optimal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Section 4.3 - Fine Tune the Model to Find the Decision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 27\n",
    "\n",
    "#Set the X training and test datasets\n",
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "\n",
    "#Set the model properties\n",
    "model_dec = RandomForestClassifier(max_depth = 11, max_features = 17, n_estimators = 20, class_weight=None, criterion = 'entropy', random_state=21)\n",
    "\n",
    "modeltraintest(vartrain, vartest, y_train, y_test, model_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "\n",
    "#Set the model properties\n",
    "model_dec = RandomForestClassifier(max_depth = 11, max_features = 17, n_estimators = 100, class_weight=None, criterion = 'entropy', random_state=21)\n",
    "\n",
    "modeltraintest(vartrain, vartest, y_train, y_test, model_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi = pd.DataFrame(model_dec.feature_importances_)\n",
    "df_fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_names = pd.DataFrame(list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_imp = pd.concat([df_fi, X_names], axis = 1)\n",
    "df_feat_imp.columns = ['Importance', 'Features']\n",
    "df_feat_imp[df_feat_imp['Importance']!=0].sort_values('Importance', ascending = False)\n",
    "\n",
    "# show feature importance compared to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = [None, 'balanced', {0:1, 1:2}, {0:1, 1:3}, {0:1, 1:5}, {0:1, 1:7.5}, {0:1, 1:10}, {0:1, 1:25},  {0:1, 1:100}]\n",
    "\n",
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "\n",
    "for w in cw:\n",
    "    print('----------------------')\n",
    "    vartitle = \"Model with Class Weight: \" + str(w) \n",
    "    varcw = w\n",
    "    #Set the model properties\n",
    "    model = RandomForestClassifier(max_depth = 11, max_features = 17, n_estimators = 20, \n",
    "                                   class_weight=varcw, criterion = 'entropy', random_state=21)\n",
    "    print(vartitle)\n",
    "    print('')\n",
    "    shorttraintest(vartrain, vartest, y_train, y_test, model)\n",
    "\n",
    "\n",
    "          \n",
    "print('----------------------')\n",
    "\n",
    "# include class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.4 - Comments throughout Section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models mostly hovered within a tenth of a percent of each other for the raw, optimal, and decision models, but the decision model performed the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 - Cost Benefit Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5.1 - Create a Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # to build a classification tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split # to split data into training and testing sets\n",
    "from sklearn.model_selection import cross_val_score # for cross validation\n",
    "from sklearn.metrics import confusion_matrix, classification_report # to create a confusion matrix and classification report\n",
    "from sklearn.metrics import plot_confusion_matrix # to draw a confusion matrix\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree # to draw a classification tree\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.fit(X_train)\n",
    "X_train_sc = sc.transform(X_train)\n",
    "X_train_sc = pd.DataFrame(X_train_sc, columns=X_train.columns)\n",
    "\n",
    "X_test_sc = sc.transform(X_test)\n",
    "X_test_sc = pd.DataFrame(X_test_sc, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [\n",
    "         \n",
    "#Logistic Regression - 2 models\n",
    "         (X_train_sc, X_test_sc, y_train,'log_opt','Logistic',\n",
    "          LogisticRegression(C=0.1, class_weight = None, penalty = 'l1', solver = 'liblinear', random_state=21)), \n",
    "         (X_train_sc, X_test_sc, y_train,'log_dec', 'Logistic',\n",
    "          LogisticRegression(C=0.1, class_weight = {0: 1, 1: 4}, penalty = 'l1', solver = 'liblinear', random_state=21)), \n",
    "         \n",
    "#Decision Tree - 2 models\n",
    "         (X_train, X_test, y_train,'dt_opt', 'DecisionTree',\n",
    "          DecisionTreeClassifier(max_depth = 9, max_leaf_nodes = 44, class_weight=None, criterion = 'gini', random_state = 21)), \n",
    "         (X_train, X_test, y_train,'dt_dec', 'DecisionTree',\n",
    "          DecisionTreeClassifier(max_depth = 9, max_leaf_nodes = 44, class_weight={0: 1, 1: 3}, criterion = 'gini', random_state = 21)),\n",
    "\n",
    "#Random Forest -2 models\n",
    "         (X_train, X_test, y_train,'rf_opt', 'RandomForest',\n",
    "          RandomForestClassifier(max_depth = 11, max_features = 17, n_estimators = 20, class_weight=None, criterion = 'entropy', random_state=21)), \n",
    "         (X_train, X_test, y_train,'rf_dec', 'RandomForest',\n",
    "          RandomForestClassifier(max_depth = 17, max_features = 12, n_estimators = 20, class_weight={0: 1, 1: 7.5}, criterion = 'entropy', random_state=21)),\n",
    "]\n",
    "          \n",
    "cm_all = pd.DataFrame(columns=['Model', 'Type','pred_noburg', 'pred_burg', \n",
    "'Score', 'Recall', 'Precision', 'F1'])\n",
    "\n",
    "\n",
    "for tr, tst, yt, n, mod, m in model:\n",
    "    m.fit(tr, yt) \n",
    "    model_pred = m.predict(tst)\n",
    "    model_prob = m.predict_proba(tst)\n",
    "    score = m.score(tst, y_test)\n",
    "    score_format = '{0:.4f}'.format(score)\n",
    "\n",
    "    \n",
    "    recall = recall_score(y_test, model_pred)\n",
    "    recall_format = '{0:.4f}'.format(recall)\n",
    "\n",
    "    f1 = f1_score(y_test, model_pred)\n",
    "    f1_format = '{0:.4f}'.format(f1)\n",
    "    \n",
    "    precision = precision_score(y_test, model_pred)\n",
    "    precision_format = '{0:.4f}'.format(precision)\n",
    "\n",
    "    y_pred_prob = m.predict_proba(tst)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "    exec(f'fpr_{n} = fpr')\n",
    "    exec(f'tpr_{n} = tpr')\n",
    "    exec(f'thresholds_{n} = thresholds')\n",
    "    exec(f'{n} = n')\n",
    "    \n",
    "    \n",
    "    cm = pd.DataFrame(confusion_matrix(y_test, model_pred))\n",
    "    cm=cm.rename(columns = {0:'pred_noburg', 1:'pred_burg'})\n",
    "    \n",
    "    exec(f'cm_{n} = cm')\n",
    "    cm['Model'] = mod\n",
    "    cm['Type'] = n\n",
    "    cm['Score'] = score_format\n",
    "    cm['Recall'] = recall_format\n",
    "    cm['Precision'] = precision_format\n",
    "    cm['F1'] = f1_format\n",
    "\n",
    "    \n",
    "    cm_all = pd.concat([cm_all, cm], axis=0)  \n",
    "    print(n + \" - Score: \" + str(score_format) +  \" - Recall: \" + \n",
    "str(recall_format) + \" - Precision: \" + str(precision_format) + \" - F1: \" +\n",
    "str(f1_format))    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    \n",
    "cm_all = cm_all.reset_index()\n",
    "cm_all['index'] = np.where(cm_all['index']==0, 'no_burglary', 'burglary')\n",
    "cm_all = cm_all.rename(columns={'index':'actual'})    \n",
    "    \n",
    "display(cm_all)\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "# Summary table of the 6 chosen models using prameters from the optimal \n",
    "# and decision models from Section 5 and HW04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5.2 - Understanding the Additional Officer Hours for Making a Wrong Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numbers for  a confusion matrix for the optimal decision tree model are as follows:\n",
    "\n",
    "-False Negative - 359 X 3 = 1077 additional hours\n",
    "\n",
    "-False Positive - 303 X 1 = 303 additional hours\n",
    "\n",
    "Total additional hrs for opt decision tree = 1380"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5.3 - Creating the Additional Officer Hours for Making a Wrong Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_all['noburglary_hrs'] = np.where(cm_all['actual']=='no_burglary', cm_all['pred_burg']*1, 0)\n",
    "cm_all['burglary_hrs'] = np.where(cm_all['actual']=='burglary', cm_all['pred_noburg']*3, 0)\n",
    "cm_all\n",
    "\n",
    "# Create two new columns showing the amount of officer allocated \n",
    "# hours for no burlaries and burglaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_all['pred_noburg'] = cm_all['pred_noburg'].astype(float)\n",
    "cm_all['pred_burg'] = cm_all['pred_burg'].astype(float)\n",
    "cm_all['Score'] = cm_all['Score'].astype(float)\n",
    "cm_all['Recall'] = cm_all['Recall'].astype(float)\n",
    "cm_all['Precision'] = cm_all['Precision'].astype(float)\n",
    "cm_all['F1'] = cm_all['F1'].astype(float)\n",
    "cm_all['noburglary_hrs'] = cm_all['noburglary_hrs'].astype(int)\n",
    "cm_all['burglary_hrs'] = cm_all['burglary_hrs'].astype(int)\n",
    "cm_all.info()\n",
    "\n",
    "# Change Dtypes in order to perform calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5.4 - Group Additional Officer Hours for Making a Wrong Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_all_ah = cm_all.groupby('Type')[['noburglary_hrs', 'burglary_hrs']].sum().reset_index()\n",
    "cm_all_ah\n",
    "\n",
    "# Sum the hours for no burglary or burglary and reduce the number of columns by half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_all_score = cm_all.groupby('Type')[['Score', 'Recall', 'Precision', 'F1']].mean().reset_index()\n",
    "cm_all_score\n",
    "\n",
    "# perform a calculation to find the mean of the listed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_all_model = cm_all.groupby('Type')['Model'].first().reset_index()\n",
    "cm_all_model\n",
    "\n",
    "# create a groupby of type based on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cost = pd.merge(cm_all_score, cm_all_ah, on='Type', how='inner')\n",
    "model_cost = pd.merge(cm_all_model, model_cost, on='Type', how='inner')\n",
    "model_cost\n",
    "\n",
    "# merge columns to create updated summary table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cost['Pred_hrs'] = model_cost['noburglary_hrs'] + model_cost['burglary_hrs']\n",
    "model_cost\n",
    "\n",
    "# create a new column showing the total predicted cost by adding\n",
    "# no burglary hours to burglary hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6 - Summarize and Visualize the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = model_cost.sort_values(by='Pred_hrs', ascending=True)\n",
    "model_result\n",
    "\n",
    "# Sort data in ascending order. This shows the optimal number of allocated\n",
    "# hours spent per model. Note that dt_dec is the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "ax = sns.barplot(y = \"Pred_hrs\", x = \"Type\",  data = model_result, palette = 'deep', dodge=False)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize='10')\n",
    "\n",
    "# bar plot of the above data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "vartrain = X_train\n",
    "vartest = X_test\n",
    "\n",
    "model_dt_dec = DecisionTreeClassifier(max_depth = 9, max_leaf_nodes = 44, class_weight={0: 1, 1: 3}, criterion = 'gini', random_state = 21)\n",
    "model_dt_dec.fit(vartrain, y_train)\n",
    "\n",
    "#Predict with the model\n",
    "model_pred = model_dt_dec.predict(vartest)\n",
    "model_prob = model_dt_dec.predict_proba(vartest)\n",
    "\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "cm_best = confusion_matrix(y_test, model_pred)\n",
    "print(cm_best)\n",
    "print(\"\")\n",
    "\n",
    "#Assess with the model\n",
    "\n",
    "print('')\n",
    "print('-----------------------------------------------------------------')\n",
    "print('----------------------CLASSIFICATION REPORT----------------------')\n",
    "print('-----------------------------------------------------------------')\n",
    "print(classification_report(y_test, model_pred))\n",
    "print('-----------------------------------------------------------------')\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.title('Decision Tree - {1: 3.5} - CM', fontweight='bold', color = 'black', fontsize='16', horizontalalignment='center')\n",
    "chart = sns.heatmap(cm_best, annot=True, cmap=\"Blues\", annot_kws={\"size\": 16}, fmt=\"g\")\n",
    "chart.set_xlabel('Predicted', fontsize=15)\n",
    "chart.set_ylabel('True', fontsize=15)\n",
    "chart.xaxis.set_ticklabels(['Good', 'Bad'], fontsize=12)\n",
    "chart.yaxis.set_ticklabels(['Good', 'Bad'], fontsize=12, va='center')\n",
    "\n",
    "plt.subplot(122)\n",
    "rfd_fpr, rfd_tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.plot(rfd_fpr, rfd_tpr, label='Decision Tree')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Decision Tree ROC Curve')\n",
    "plt.show();\n",
    "\n",
    "# Show a confusion matrix for the best model, show classification report,\n",
    "# show heat map version of confusion matrix, and ROC curve of optimal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose this as the best model because it predicts the minimum amount of additional officer hours of all the models.\n",
    "\n",
    "The confusion matrix shows that while there are a lot of false positives, the low number of false negatives optimizes the amount of officer hours for the department. \n",
    "\n",
    "The model shows that it is better to predict a burglary and spend the officer hours preventing one, than to predict there will not be a burglary and have to spend more officer hours solving it. Although I think it needs some work in making accurate predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7 - Predict Week 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canton_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canton_new['BurgStatus'].value_counts()\n",
    "\n",
    "# data from week 49 shows 19 no burglaries and 4 burglaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_new_test = df_canton_new['BurgStatus']\n",
    "vartest = df_canton_new.drop(['Subzone','YEAR_WEEK', 'SUB_YEAR_WEEK', 'All_comp',\n",
    "       'FalseAlarm_comp', 'Arrest_comp', 'Cleared_comp', 'NoContact_comp',\n",
    "       'NoReport_comp', 'Resolved_comp', 'Filed_comp', 'Calls_comp',\n",
    "       'BurgAlarm_comp', 'Suspicious_comp', 'Shots_comp', 'Intox_comp',\n",
    "       'Drugs_comp', 'Assault_comp', 'Armed_comp', 'Disturb_comp',\n",
    "       'Fireworks_comp', 'Noise_comp', 'Stalking_comp', 'ActualBurg',\n",
    "       'BurgStatus', 'BurgStatus2','Friday', 'Monday', 'Saturday',\n",
    "       'Sunday', 'Thursday', 'Tuesday', 'Wednesday', 'month_1', 'month_2',\n",
    "       'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
    "       'month_9', 'month_10', 'month_11', 'month_12','call_FW FIREWORKS','disp_Runaway juvenile (entered NCIC)',\n",
    "       'disp_SAT-SETTLED AMONG SELVES', 'disp_TES-TEST',\n",
    "       'disp_TI -TOW IN', 'disp_Truancy', 'disp_VA Hospital Alarm (Fire)'],axis=1)\n",
    "\n",
    "model = model_dt_dec\n",
    "model.fit(vartrain, y_train)\n",
    "\n",
    "model_pred = model.predict(vartest)\n",
    "model_prob = model.predict_proba(vartest)\n",
    "print('Confusion Matrix:')\n",
    "cm = confusion_matrix(y_new_test, model_pred)\n",
    "print(cm)\n",
    "print(\"\")\n",
    "\n",
    "#Assess with the model\n",
    "print('')\n",
    "print('-----------------------------------------------------------------')\n",
    "print('----------------------CLASSIFICATION REPORT----------------------')\n",
    "print('-----------------------------------------------------------------')\n",
    "print(classification_report(y_new_test, model_pred))\n",
    "print('-----------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "\n",
    "\n",
    "plt.title('New Customers', fontweight='bold', color = 'black', fontsize='16', horizontalalignment='center')\n",
    "chart = sns.heatmap(cm, annot=True, cmap=\"Blues\", annot_kws={\"size\": 16}, fmt=\"g\")\n",
    "chart.set_xlabel('Predicted', fontsize=15)\n",
    "chart.set_ylabel('True', fontsize=15)\n",
    "chart.xaxis.set_ticklabels(['Good', 'Bad'], fontsize=12)\n",
    "chart.yaxis.set_ticklabels(['Good', 'Bad'], fontsize=12, va='center')\n",
    "\n",
    "# fit new data to best model and run confusion matrix, classification report,\n",
    "# and heat map.\n",
    "# Note that the model predicted perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred = pd.DataFrame(model_pred, columns= ['Pred'])\n",
    "model_prob = pd.DataFrame(model_prob, columns= ['Prob_Good', 'Prob_Bad'])\n",
    "\n",
    "# set data frames for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canton_new=df_canton_new.reset_index()\n",
    "df_canton_new\n",
    "\n",
    "# set data frames for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canton_final=df_canton_new[['Subzone','BurgStatus']]\n",
    "\n",
    "# set dataframes for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canton_final\n",
    "\n",
    "# set dataframes for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_canton_final, model_pred, model_prob], axis = 1)\n",
    "\n",
    "# finally, concat data frames for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final\n",
    "\n",
    "# show final model first draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df_final.set_index('Subzone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final\n",
    "\n",
    "# final model showing matching burgstatus and pred colums for an accurate \n",
    "# prediction as well as the probabilities for further scrutiny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the data and the model, there was a predicted burglary at subzones 'ZONE1B', 'ZONE3E', 'ZONE5A', and 'ZONE5B'. According to the model, Chief Croft should allocate officer hours to the subzones mentioned in order to stop a burglary next week.\n",
    "\n",
    "This was a good prediction. The confusion matrix shows that the model correctly predicted all burglaries and non burglaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
